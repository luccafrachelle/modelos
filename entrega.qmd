---
title: "Entrega trabajo final modelos lineales"
format: pdf
author: "Lucca Frachelle , Valentina Solidni , Cecilia Waksman"
date: "Sys.Date()"
editor: visual
---

# Introducción

Este trabajo consiste en un análisis de la incidencia de ciertos factores sobre la presión sanguínea en personas hipertensas. Los factores que se tomarán en cuenta para ello son: edad, peso, superficie corporal, duración desde que a la persona le diagnosticaron hipertensión, el pulso en estado basal y un índice de estrés.

Para ello se utilizarán df de un estudio realizado en una policlínica universitaria y se realizará un modelo de regresión lineal múltiple, dónde la variable de respuesta, Y, es la presión arterial (*presion_art*) y las variables explicativas serán seleccionadas de las nombradas anteriormente (las cuales formarán la matriz X).

El modelo se podrá escribir como $Y=X\beta + \epsilon$, donde $\beta$ es un vector de parámetros a estimar.

# Supuestos a cumplir

Para poder obtener conclusiones confiables, el modelo debe cumplir con determinados supuestos

-   no multicolinealidad: exacta ni aproximada, para asegurar que la matriz X sea de rango completo (conformable),

-   linealidad: la relación entre variables expicativas y la respuesta debe ser aproximadamente lineal,

-   homoscedasticidad: la varianza de los errores no depende de ninguna de las variables explicativas,

-   normalidad: los errores del modelo deben presentar una distribución normal,

-   atípicos/influyentes: si bien no es un supuesto en si mismo, es recomendable identificar observaciones atípicas e influyentes al modelo.

# Librerias

```{r}
#| output: TRUE
library(readxl)
library(tidyverse)
library(GGally)
library(car)
library(skedastic)
library(robustbase)
library(tseries)
```

```{r}
#df <- read_excel("data/datos_presion.xlsx")
df = read_excel('./data/datos_presion.xlsx')
df <- df %>% dplyr::select(-id)
```

# Análisis exploratorio

```{r}
summary(df)
```

```{r}
ggpairs(df)
```

```{r}
mod = lm(presion_art ~ ., data = df)
```

## Diagnostico del modelo

## Multicolinealidad

```{r}
vif(mod)
```

Se observa que al hacer el vif la variable que tenía un valor mas alto indicando una alta multicolinealidad era la variable "peso", por esta razón fue que se descidio eliminarla del modelo. Se elimina la variable peso que tiene un el VIF mas alto

```{r}
mod = update(mod, . ~ . - peso)
summary(mod)
```

```{r}
vif(mod)
```

Todos los VIF son con diferencia menoroes a 5, por lo que no hay multicolinealidad. Se puede seguir con el analisis. Si observamos en el analisis multivariado , la variable peso y superficie corporal tienen una correlacion muy alto, por lo que tiene sentido multicolineanidad en el modelo.

## Lineanidad

```{r}
crPlots(mod)
```

Parece no haber problemas de linealidad en el modelo. Ya que los residuos no presentan un patron claro , como dispersión o curvatura a lo largo de los valores de x. Quizas puede llegar a dar problemas la eadad linealindad. Se va a seguir con el analisis sin modificarla esperando que no sea un problema.

## Homoscedasticidad

Atípicos

```{r}
res <- rstudent(mod)
yhat <- fitted(mod)

which(abs(res)>3)

library(ggplot2)
ggplot(mod, aes(x=yhat, y= res))+
  geom_point()+
  geom_hline(yintercept = -3,color="darkblue")+
  geom_hline(yintercept = 3,color="darkblue")
```

```{r}
ncvTest(mod)
```

```{r}
breusch_pagan(mod)
```

Los dos test tienen un p-value mayor a 0.7 por lo que no se rechaza la hipotesis nula. Es decir, no hay evidencia para probar que la varianza de los errores no son iguals. Por ende no hay problemas de homocedasticidad.

## Normalidad de los residuos

```{r}
plot(density(rstudent(mod)))
```

```{r}
n <- nrow(df)
z_i <- qnorm(seq(n)/(n + 1))
qq <- data.frame(teoricos = z_i,
                 empiricos = sort(rstudent(mod)))
library(ggplot2)
ggplot(qq, aes(x = teoricos, y = empiricos)) +
  geom_point() +
  xlab('Cuantiles teoricos') +
  ylab('Cuantiles empiricos') +
  geom_abline(slope = 1, intercept = 0, col = 2, size = 1.5)#comparación entre disrtib de los df y distrib normal

```

```{r}
shapiro.test(rstudent(mod))
jarque.bera.test(rstudent(mod))#rechazo al 5
ks.test(rstudent(mod), 'pnorm')

```

El test Jarque-Bera tiene un p-value menor que 0.05, por lo que se rechaza la hipotesis nula de que los residuos son normales. Sin embargo, el test de Shapiro-Wilk y el test de Kolmogorov-Smirnov no rechazan la hipotesis nula. Por lo que no se puede afirmar que los residuos no son normales. Se va a intervenir el modelo para ver si se puede mejorar la normalidad de los residuos. Como se menciono anteriormente la variable edad puede no ser lineal, por lo que se va a transformar la variable edad para ver si se puede mejorar la normalidad de los residuos.

## Presencia de Influyentes

```{r}
h_i <- influence(mod)$hat
D_i <- cooks.distance(mod)
df_influencia <- data.frame(i = 1:nrow(df),
                            h_i = h_i,
                            D_i = D_i)
```

```{r}
ggplot(df_influencia, aes(x = i, y = h_i)) +
  geom_point() +
  geom_segment(aes(x = i, xend = i, y = 0, yend = h_i)) +
  xlab('') +
  ylab(expression(h[i])) +
  geom_abline(slope = 0, intercept = 2*length(coefficients(mod))/nrow(df), col = 2, linetype = 'dashed') +
  ggtitle('Leverage')
```

```{r}
ggplot(df_influencia, aes(x = i, y = D_i)) +
  geom_point() +
  geom_segment(aes(x = i, xend = i, y = 0, yend = D_i)) +
  xlab('') +
  ylab(expression(D[i])) +
  geom_abline(slope = 0, intercept = 4/nrow(df), col = 2, linetype = 'dashed') +
  ggtitle('Distancia de Cook')

```

Intervención de influyentes según Levrage

```{r}
df$I38 <- 0
df$I38[38] <- 1
df$I73 <- 0
df$I73[73] <- 1
mod_I <- lm(presion_art ~ . -peso, data = df)
vif(mod_I)
#obs 58 es influyente en ambos gráficos pero aceptamos normalidad si lo quitamos
```

```{r}
crPlots(mod_I)
```

```{r}
ncvTest(mod_I)
breusch_pagan(mod_I)
```

```{r}
shapiro.test(rstudent(mod_I))
jarque.bera.test(na.omit(rstudent(mod_I)))
ks.test(rstudent(mod_I), 'pnorm')

```

Interviención de influyentes según distancia de Cook

```{r}
df$I62 <- 0
df$I62[62] <- 1
mod_II <- lm(presion_art ~ . -peso -I38 -I73, data = df)
vif(mod_II)
# si se borra intervención de influyentes de Leverage, debemos actualizar nuestro modelo borradno "-I38 -I73".
```

```{r}
crPlots(mod_II)
```

```{r}
ncvTest(mod_II)
```

```{r}
breusch_pagan(mod_II)
```

```{r}
shapiro.test(rstudent(mod_II))
jarque.bera.test(na.omit(rstudent(mod_II)))
ks.test(rstudent(mod_II), 'pnorm')
```

Una vez habiendo sacado los df 38, 73, la variable peso e interviniendo el dato 62, fue recien ahi donde se complieron todos los supuestos de forma correcta que era lo que necesitabamos para tener el mejor modelo posible.


Tests de significación individual y global

```{r}
summary(mod_II)
# Intervención significativa
# duracion_hip y stress son no significativas
```

Modelo sin variables no significativas

```{r}
mod_II2 = update(mod_II, . ~ . - peso - stress -duracion_hip)
#modelo sin stress y/o duracion_hip no acepta normalidad
summary(mod_II2)
```


```{r}
shapiro.test(rstudent(mod_II2))
jarque.bera.test(na.omit(rstudent(mod_II2)))
ks.test(rstudent(mod_II2), 'pnorm')
```

Sin variables no significativas

```{r}
mod2 <- lm(presion_art~.-peso-stress-duracion_hip, data=df)
vif(mod2)
```

```{r}
summary(mod2)
```

```{r}
h_i2 <- influence(mod2)$hat
D_i2 <- cooks.distance(mod2)
df_influencia2 <- data.frame(i = 1:nrow(df),
                            h_i = h_i2,
                            D_i = D_i2)
```

```{r}
ggplot(df_influencia2, aes(x = i, y = h_i)) +
  geom_point() +
  geom_segment(aes(x = i, xend = i, y = 0, yend = h_i)) +
  xlab('') +
  ylab(expression(h[i])) +
  geom_abline(slope = 0, intercept = 2*length(coefficients(mod2))/nrow(df), col = 2, linetype = 'dashed') +
  ggtitle('Leverage')
```

```{r}
ggplot(df_influencia2, aes(x = i, y = D_i)) +
  geom_point() +
  geom_segment(aes(x = i, xend = i, y = 0, yend = D_i)) +
  xlab('') +
  ylab(expression(D[i])) +
  geom_abline(slope = 0, intercept = 4/nrow(df), col = 2, linetype = 'dashed') +
  ggtitle('Distancia de Cook')
```

```{r}
df_influencia2 %>% arrange(desc(D_i)) %>% head(5) %>% pull(i)
df_influencia2 %>% arrange(desc(h_i)) %>% head(5) %>% pull(i)
```

Atípicos

```{r}
res <- rstudent(mod2)
yhat <- fitted(mod2)

which(abs(res)>=3)

library(ggplot2)
ggplot(mod2, aes(x=yhat, y= res))+
  geom_point()+
  geom_hline(yintercept = -3,color="darkblue")+
  geom_hline(yintercept = 3,color="darkblue")

#Obs 62 
```

Quitando obs atípica 62

```{r}
df <- df[-62,]
mod_sin62 <- lm(presion_art ~ .-peso- stress -duracion_hip , data=df)
shapiro.test(rstudent(mod_sin62))
jarque.bera.test(na.omit(rstudent(mod_sin62)))
ks.test(rstudent(mod_sin62), 'pnorm')#no acepta normalidad
```

## Randomize

```{r}
# aleatoricemos 'P' veces
P <- 1000

# funcionas auxiliares
h <- function(y, x, X){
  # Esta funcion auxiliar 'remueve' el efecto de las columnas
  # de X en x y en Y
  if (class(X) != 'matrix') X <- as.matrix(X)
  H <- X%*%solve(t(X)%*%X)%*%t(X)
  y1 <- y - y%*%H
  x1 <- x - x%*%H
  return(list(ye    = as.numeric(y1),
              equis = as.numeric(x1)))
}
estima <- function(h){
  # Esta funcion auxiliar estima el parametro de la pendiente
  # en una RLS que no incluye constante.
  # Usa como insumo el resultado de la funcion h()
  sum(h$ye*h$equis)/sum(h$equis^2)
}
desvio <- function(h){
  # Esta funcion auxiliar estima el desvio del estimador de la pendiente
  # en un modelo de RLS que no incluye la constante.
  # Usa como insumo el resultado de la funcion h()
  b <- sum(h$ye*h$equis)/sum(h$equis^2)
  r <- h$ye - b*h$equis
  n <- length(h$ye)
  sqrt((sum(r^2)/(n-1))/sum(h$equis^2))
}
```

```{r}
est <- matrix(NA, P, 3)

for (i in 1:P){
  # se quita el efecto de las demas variables...
  h1 <- h(df$presion_art, df$edad, cbind(1,df[,c('sup_corp','pulso')]))
  h2 <- h(df$presion_art, df$sup_corp, cbind(1,df[,c('edad','pulso')]))
  h3 <- h(df$presion_art, df$pulso, cbind(1,df[,c('edad','sup_corp')]))
  
  # se lleva a cabo la permutacion aleatoria...
  h1$ye <- sample(h1$ye)
  h2$ye <- sample(h2$ye)
  h3$ye <- sample(h3$ye)
  
  # estima...
  b1 <- estima(h1)
  b2 <- estima(h2)
  b3 <- estima(h3)
  
  # se obtiene el desvio del estimador
  sb1 <- desvio(h1)
  sb2 <- desvio(h2)
  sb3 <- desvio(h3)
  
  # y se calcula el estadistico 't'
  t1 <- b1/sb1
  t2 <- b1/sb2
  t3 <- b1/sb3
  
  # los almacenamos
  est[i,] <- c(t1, t2, t3)
}

# p-valor edad
# hist(est[,1])
t_1 <- coef(summary(mod2))[2,3]
# abline(v = t_1, col = 2, lwd = 2) #estimación de t student en edad, mod2
pv1 <- mean(est[,1]>abs(t_1)|est[,1]< -abs(t_1))

# p-valor sup_corp
#hist(est[,2])
t_2 <- coef(summary(mod2))[3,3]
#abline(v = t_2, col = 2, lwd = 2)
pv2 <- mean(est[,2]>abs(t_2)|est[,2]< -abs(t_2))


# p-valor pulso
#hist(est[,3])
t_3 <- coef(summary(mod2))[4,3]
#abline(v = t_3, col = 2, lwd = 2)
pv3 <- mean(est[,3]>abs(t_3)|est[,3]< -abs(t_3))
```

```{r}
summary(mod2)
c(pv1,pv2,pv3)
```

```{r}

# Una funcion que haga todo el trabajo
randomize <- function(mod, P = 1000){
  # Esta funcion aproxima los p-valores de las pruebas de significacion
  # individual mediante 'P' permutaciones aleatorias 
  t_stat <- coef(summary(mod))[,3]
  k   <- length(t_stat)
  est <- matrix(NA, P, k)
  mm  <- mod$model
  Y   <- mm[,1]
  X   <- cbind(1,mm[,-1])
  
  for (i in 1:P){
    tes <- rep(NA, k)
    for (j in 1:k){
      # se quita el efecto de las demas variables
      h1 <- h(Y, X[,j], X[,-j])
      # se permuta
      h1$ye <- sample(h1$ye)
      # se estima el parametro
      b1 <- estima(h1)
      # se estima el desvio
      sb1 <- desvio(h1)
      # se almacena el estadistic 't'
      tes[j] <- b1/sb1 
    }
    est[i,] <- tes
  }
  # se calculan los p-values bilaterales
  t_stat <- matrix(rep(abs(t_stat), P),nrow = P, byrow = TRUE)
  pv <- apply(est > t_stat, 2, mean)
  pv <- pv + apply(est < -t_stat,2,mean)
  
  smod <- coef(summary(mod))
  smod <- cbind(smod, pv)
  colnames(smod)[5] <- 'Pr(>|t|)_rand'
  return(smod)
}
set.seed(2100)
round(randomize(mod2),6)
```

Modelo con variables significativas (rechazo pruebas de significión individual)


## Evaluación del desempeño predictivo del modelo

```{r}
n=nrow(df)
pred <- rep(NA,n)

for(i in 1:n){
  df_i <- df[-i,]
  mod_i <- update(mod2, data = df_i)
  pred[i] <- predict(mod_i, newdata = df[i,])
}

#R^2 para las predicciones del Leave one out

R2 <- 1-((sum((df$presion_art - pred)^2))/ (sum((df$presion_art - mean(df$presion_art))^2)))
R2

summary(mod2)$r.squared
```
