---
title: "Entrega trabajo final modelos lineales"
format: pdf
author: "Lucca Frachelle , Valentina Solidni , Cecilia Waksman"
date: "Sys.Date()"
editor: visual
---


# Carga de datos y librerias

```{r}
#| output: TRUE
library(readxl)
library(tidyverse)
library(GGally)
library(car)
library(skedastic)
library(robustbase)
```


```{r}
df = read_excel('./data/datos_presion.xlsx')
df <- df %>% dplyr::select(-id)
```

# Análisis exploratorio

```{r}
ggpairs(df)
```

```{r}
summary(df)
```




```{r}
mod = lm(presion_art ~ ., data = df)
```
## Diagnostico del modelo

## Multicolinealidad
```{r}
vif(mod)
```
Se elimina la variable peso que tiene un el VIF mas alto

```{r}
mod = update(mod, . ~ . - peso)
```

```{r}
vif(mod)
```
Todos los VIF son con diferencia menoroes a 5, por lo que no hay multicolinealidad. Se puede seguir con el analisis.
Si observamos en el analisis multivariado , la variable peso y superficie corporal tienen una correlacion muy alto, por lo que tiene sentido multicolineanidad en el modelo.

## Lineanidad
```{r}
crPlots(mod)
```
Parece no haber problemas de linealidad en el modelo. Ya que los residuos no presentan un patron claro , como dispersión o curvatura a lo largo de los valores de x. Quizas puede llegar a dar problemas la eadad linealindad. Se va a seguir con el analisis sin modificarla esperando que no sea un problema.

## Homocedasticidad
```{r}
ncvTest(mod)
breusch_pagan(mod)
```
Los dos test tienen un p-value mayor a 0.7 por lo que no se rechaza la hipotesis nula. Es decir, no hay evidencia para probar que la varianza de los errores no son iguals. Por ende no hay problemas de homocedasticidad.

## Normalidad de los residuos
```{r}
shapiro.test(residuals(mod))
tseries::jarque.bera.test(rstudent(mod))
ks.test(rstudent(mod), 'pnorm')
```
El test Jarque-Bera tiene un p-value menor que  0.05, por lo que se rechaza la hipotesis nula de que los residuos son normales. Sin embargo, el test de Shapiro-Wilk y el test de Kolmogorov-Smirnov no rechazan la hipotesis nula. Por lo que no se puede afirmar que los residuos no son normales. Se va a intervenir el modelo para ver si se puede mejorar la normalidad de los residuos.
Como se menciono anteriormente la variable edad puede no ser lineal, por lo que se va a transformar la variable edad para ver si se puede mejorar la normalidad de los residuos.


## Tranformacion de la variable edad

### Transofrmacion logaritmica
```{r}
df$log_edad <- log(df$edad)
mod_log_edad = lm(presion_art ~ ., data = df)
mod_log_edad = update(mod_log_edad, . ~ . - peso)
```

```{r}
shapiro.test(residuals(mod_log_edad))
tseries::jarque.bera.test(rstudent(mod_log_edad))
ks.test(rstudent(mod_log_edad), 'pnorm')

```




```{r}
df$sqrt_edad <- sqrt(df$edad)
mod_sqrt_edad = lm(presion_art ~ ., data = df)
mod_sqrt_edad = update(mod_sqrt_edad, . ~ . - log_edad , . ~ . - peso)
```


```{r}
shapiro.test(residuals(mod_sqrt_edad))
tseries::jarque.bera.test(rstudent(mod_sqrt_edad))
ks.test(rstudent(mod_sqrt_edad), 'pnorm')

```
Las dos transformaciones no tienen problemas de normalidad en los residuos. Se va a elegir la transformacion de la raiz queadrada ya que tiene un p-value mayor en el test de Jarque-Bera.

## Presencia de atipicos
```{r}
h_i <- influence(mod_sqrt_edad)$hat
D_i <- cooks.distance(mod)
df_influencia <- data.frame(i = 1:nrow(df),
                            h_i = h_i,
                            D_i = D_i)
```

```{r}
ggplot(df_influencia, aes(x = i, y = h_i)) +
  geom_point() +
  geom_segment(aes(x = i, xend = i, y = 0, yend = h_i)) +
  xlab('') +
  ylab(expression(h[i])) +
  geom_abline(slope = 0, intercept = 2*length(coefficients(mod))/nrow(df), col = 2, linetype = 'dashed') +
  ggtitle('Leverage')
```
```{r}
ggplot(df_influencia, aes(x = i, y = D_i)) +
  geom_point() +
  geom_segment(aes(x = i, xend = i, y = 0, yend = D_i)) +
  xlab('') +
  ylab(expression(D[i])) +
  geom_abline(slope = 0, intercept = 4/nrow(df), col = 2, linetype = 'dashed') +
  ggtitle('Distancia de Cook')

```
```{r}
influyentes <- df_influencia %>%
  filter(D_i > 4 / nrow(df)) %>% arrange(desc(D_i)) %>% head(1)
```
Se interviene el modelo poniendo indicatrices para las obs

```{r}
df$influyente <- 0
df$influyente[influyentes$i] <- 1
```
Se evalua el modelo intervenido
```{r}
mod_intervenido = lm(presion_art ~ . + influyente, data = df)
mod_intervenido = update(mod_intervenido, . ~ . - peso , . ~ . - log_edad , . ~ . - edad)
```

Se analiza la influencia de atipicos en el modelo intervenido
```{r}
h_i <- influence(mod_intervenido)$hat
D_i <- cooks.distance(mod_intervenido)
df_influencia <- data.frame(i = 1:nrow(df),
                            h_i = h_i,
                            D_i = D_i)
```

```{r}
ggplot(df_influencia, aes(x = i, y = h_i)) +
  geom_point() +
  geom_segment(aes(x = i, xend = i, y = 0, yend = h_i)) +
  xlab('') +
  ylab(expression(h[i])) +
  geom_abline(slope = 0, intercept = 2*length(coefficients(mod_intervenido))/nrow(df), col = 2, linetype = 'dashed') +
  ggtitle('Leverage')
```
```{r}
ggplot(df_influencia, aes(x = i, y = D_i)) +
  geom_point() +
  geom_segment(aes(x = i, xend = i, y = 0, yend = D_i)) +
  xlab('') +
  ylab(expression(D[i])) +
  geom_abline(slope = 0, intercept = 4/nrow(df), col = 2, linetype = 'dashed') +
  ggtitle('Distancia de Cook')

```

Se observa que el atipico influye bastante. Por mas de que siguen habiendo mas atipicos en el modelo, se decide seguir con el analisis ya que no se observa que influyan tanto como el atipico que se intervino

## Resultados del modelo final
```{r}
mod_intervenido %>% summary()
```
Viendo el resultado del summary , que tiene como unica variable super siginificativa a la variable superficie corporal. Se explora un modelo lineal simple con la variable superficie corporal.

```{r}
mod_simple = lm(presion_art ~ sup_corp, data = df)
```

```{r}
mod_simple %>% summary()
```
Se pierde un 13% de la varianza explicada, pero se obtiene un modelo mas simple y con una variable significativa. Se va a analizar 
```{r}
```

